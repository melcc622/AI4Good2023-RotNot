# -*- coding: utf-8 -*-
"""ResNet-Model-ver02

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-JSt_YrsMjvvB95FUzazkj0J9I_HEzfZ
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.

import os
import torch
import torchvision
import csv
import torch.nn as nn
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, ConcatDataset, Dataset, random_split
import torchvision.transforms as transforms
from torchvision.io import read_image
from torchvision.utils import make_grid
from torchsummary import summary
import numpy as np
import matplotlib.pyplot as plt
from torch import optim
# %matplotlib inline

"""# ResNet Model"""

#ResNet code adapted from https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/CNN_architectures/pytorch_resnet.py

class block(nn.Module):
    def __init__(
        self, in_channels, intermediate_channels, identity_downsample=None, stride=1
    ):
        super().__init__()
        self.expansion = 4
        self.conv1 = nn.Conv2d(
            in_channels,
            intermediate_channels,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False,
        )
        self.bn1 = nn.BatchNorm2d(intermediate_channels)
        self.conv2 = nn.Conv2d(
            intermediate_channels,
            intermediate_channels,
            kernel_size=3,
            stride=stride,
            padding=1,
            bias=False,
        )
        self.bn2 = nn.BatchNorm2d(intermediate_channels)
        self.conv3 = nn.Conv2d(
            intermediate_channels,
            intermediate_channels * self.expansion,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False,
        )
        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)
        self.relu = nn.ReLU()
        self.identity_downsample = identity_downsample
        self.stride = stride

    def forward(self, x):
        identity = x.clone()

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.bn3(x)

        if self.identity_downsample is not None:
            identity = self.identity_downsample(identity)

        x += identity
        x = self.relu(x)
        return x

class ResNet(nn.Module):
    def __init__(self, block, layers, image_channels, num_classes, num_fruit_veg):
        super(ResNet, self).__init__()
        self.in_channels = 64
        self.conv1 = nn.Conv2d(
            image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False
        )
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # Essentially the entire ResNet architecture are in these 4 lines below
        self.layer1 = self._make_layer(
            block, layers[0], intermediate_channels=64, stride=1
        )
        self.layer2 = self._make_layer(
            block, layers[1], intermediate_channels=128, stride=2
        )
        self.layer3 = self._make_layer(
            block, layers[2], intermediate_channels=256, stride=2
        )
        self.layer4 = self._make_layer(
            block, layers[3], intermediate_channels=512, stride=2
        )

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fresh_rotten_head = nn.Linear(512 * 4, num_classes)
        self.type_head = nn.Linear(512 * 4, num_fruit_veg)

    def forward(self, x):
        output = self.conv1(x)
        output = self.bn1(output)
        output = self.relu(output)
        output = self.maxpool(output)
        output = self.layer1(output)
        output = self.layer2(output)
        output = self.layer3(output)
        output = self.layer4(output)

        output = self.avgpool(output)
        output = output.reshape(output.shape[0], -1)

        # output through the 2 heads
        output_fresh_rotten = self.fresh_rotten_head(output)
        output_type = self.type_head(output)
        return output_fresh_rotten, output_type

    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):
        identity_downsample = None
        layers = []

        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes
        # we need to adapt the Identity (skip connection) so it will be able to be added
        # to the layer that's ahead
        if stride != 1 or self.in_channels != intermediate_channels * 4:
            identity_downsample = nn.Sequential(
                nn.Conv2d(
                    self.in_channels,
                    intermediate_channels * 4,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                nn.BatchNorm2d(intermediate_channels * 4),
            )

        layers.append(
            block(self.in_channels, intermediate_channels, identity_downsample, stride)
        )

        # The expansion size is always 4 for ResNet 50,101,152
        self.in_channels = intermediate_channels * 4

        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,
        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,
        # and also same amount of channels.
        for i in range(num_residual_blocks - 1):
            layers.append(block(self.in_channels, intermediate_channels))

        return nn.Sequential(*layers)

# defining ResNet with 50 layers
def ResNet50(num_classes, num_fruit_veg, channels=3):
    return ResNet(block, [3,4,6,3], channels, num_classes, num_fruit_veg)

# defining ResNet with 18 layers
def ResNet18(num_classes, num_fruit_veg, channels=3):
    return ResNet(block, [2,3,2,2], channels, num_classes, num_fruit_veg)

def run_epoch(model, data_loader, loss_fnc, optimizer=None, is_train=True):
  """
  @param:
  `model`: class; the model of which we are training the data on
  `data_loader`: torch.utils.data.DataLoader Object; the object with data in a machine readable form
  `loss_func`: torch.utils.data.DataLoader Object; the loss function of our model
  `optimizer`: torch.optim Object; the optimizer for our model. None by default.
  `is_train`: Boolean; value that indicates whether model should be trained or evaluated.

  @desc: Given input parameters, the run_epoch function will calculate the
  aggregate loss of our model's epoch. The function will use the provided
  optimizer and the loss function as a loss evaluation metric to output the
  loss.

  """
  # Some layers in the model will behave differently in train vs eval mode
  model.to(device)
  if is_train:
    model.train()
  else:
    model.eval()

  # Iterate over each batch in our data loader
  epoch_loss = 0.0
  epoch_fresh_loss = 0
  epoch_type_loss = 0


  for X, fresh_rotten_label, type_label in data_loader:
  # We always want to ensure that we have cleared the stored gradients
    if is_train:
      optimizer.zero_grad()

    # Move the data to the proper device
    X = X.to(device)
    fresh_rotten_label = fresh_rotten_label.to(device).long() # revised from y = y.to(device).long()
    type_label = type_label.to(device).long()

    # report_gpu()
    # Put the data X through our model to get a prediction
    pred_fresh_rotten, pred_type_label = model.forward(X)

    # report_gpu()
    # Find the loss between the ground truth and our prediction
    fresh_loss = loss_fnc(pred_fresh_rotten, fresh_rotten_label)
    type_loss = loss_fnc(pred_type_label, type_label)

    loss = fresh_loss + type_loss

    # getting loss value
    epoch_loss += loss.item()
    epoch_fresh_loss += fresh_loss.item()
    epoch_type_loss += type_loss.item()

    # Update the model parameters using the optimizer update rule
    if is_train:
      loss.backward()
      optimizer.step()

  return epoch_loss / len(data_loader), epoch_fresh_loss / len(data_loader), epoch_type_loss / len(data_loader)

"""# Train"""

def train_network(model, train_data, validate_data, loss_fnc, num_epochs, batch_size, lr):
  """
  @param:
  `model`: class; the model of which we are training the data on
  `batch_size`: int; the batch size that you are training the data with
  `lr`: float [0,1]; the learning rate of our optimizer
  `num_epochs`: int; the number of epochs

  @desc: Given the `model` input parameter, the training_model function will load
  the training and testing datasets. The function will use the Adam optimizer and
  the cross entropy function as a loss evaluation metric to output the losses in the
  training and testing datasets along with its accuracy.

  """
  # The function will use GPU to accelerate the training if it's available.
  model.to(device)
  #device = "cuda" if torch.cuda.is_available() else "cpu"
  train_loader = DataLoader(dataset = train_data, batch_size = batch_size,
                            shuffle = True, generator=torch.Generator().manual_seed(seed))
  validate_loader = DataLoader(dataset = validate_data, batch_size = batch_size,
                               shuffle = False, generator=torch.Generator().manual_seed(seed))



  optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))
  # optimizer= optim.SGD(model.parameters(), lr=lr) # momentum = ???

  # train losses
  train_fresh_losses = []
  train_type_losses = []

  # validate losses
  valid_fresh_losses = []
  valid_type_losses = []

  # validate accuracies
  valid_accuracy_fresh_rotten = [] # validate accuracy list
  valid_accuracy_type = []

  num_epochs = num_epochs

  for epoch in range(num_epochs):
    epoch_loss_train, epoch_fresh_loss_train, epoch_type_loss_train = run_epoch(model, train_loader, loss_fnc, optimizer, device)
    # test_loss, test_acc = evaluate(test_loader, model, loss_function, device)

    train_fresh_losses.append(epoch_fresh_loss_train)
    train_type_losses.append(epoch_type_loss_train)

    with torch.no_grad():
      epoch_loss_val, epoch_fresh_loss_val, epoch_type_loss_val = run_epoch(model, validate_loader, loss_fnc, is_train=False)
      valid_fresh_losses.append(epoch_fresh_loss_val)
      valid_type_losses.append(epoch_type_loss_val)
      val_accuracy_fresh_rotten, val_accuracy_type, val_fresh_losses, val_type_losses = test_network(model, validate_data, loss_fnc) # validate accuracy value
      valid_accuracy_fresh_rotten.append(val_accuracy_fresh_rotten)
      valid_accuracy_type.append(val_accuracy_type)

    print("Epoch: {:>2}, Train Loss: {:.4f}, Validate Freshness Loss: {:.4f}, Validate Type Loss: {:.4f}, Validate Accuracy Fresh Rotten: {:.4f}, Validate Accuracy Type: {:.4f}".format(
        epoch, epoch_loss_train, epoch_fresh_loss_val, epoch_type_loss_val, val_accuracy_fresh_rotten, val_accuracy_type
    ))

    # save model
    if (epoch + 1) % 10 == 0:
      PATH = "/content/drive/Shareddrives/RotNotCrew/models/"+model_name+"-"+str(epoch+1)+".pth"
      torch.save(model.state_dict(), PATH)

  return train_fresh_losses, train_type_losses, valid_accuracy_fresh_rotten, valid_accuracy_type, valid_fresh_losses, valid_type_losses

"""# Test"""

def accuracy(outputs, labels):
  predictions = outputs.argmax(-1)
  correct = torch.sum(labels == predictions).item()
  return correct / len(labels)

def test_network(model, test_data, loss_function):
  """
  @params:
  `model`: class; model which we are testing data on
  `test_data`: datatset; the unseen portion of the dataset which we are training
  our model on

  @desc: Given our model and testing dataset, the test_network function loads
  the test set
  """
  acc_fresh_rotten = 0
  acc_type_label = 0
  loss = 0
  epoch_fresh_loss = 0
  epoch_type_loss = 0

  n_samples = 0

  test_loader = DataLoader(test_data,batch_size=1,shuffle=False)
  model.to(device)
  model.eval()

  # num_correct = 0
  # num_incorrect = 0

  # Iterate over test data; X = input image, y = target
  for X, fresh_rotten_label, type_label in test_loader:

    # torch.no_grad() tells PyTorch to not calculate gradients to save space
    with torch.no_grad():
      X = X.to(device)
      fresh_rotten_label = fresh_rotten_label.to(device).long()
      type_label = type_label.to(device).long()

      pred_fresh_rotten, pred_type_label = model.forward(X)

      # Compute the loss
      fresh_loss = loss_function(pred_fresh_rotten, fresh_rotten_label)
      type_loss = loss_function(pred_type_label, type_label)

      loss += fresh_loss + type_loss
      epoch_fresh_loss += fresh_loss
      epoch_type_loss += type_loss

      # Compute the accuracy
      acc_fresh_rotten += accuracy(pred_fresh_rotten, fresh_rotten_label)*len(fresh_rotten_label)
      acc_type_label += accuracy(pred_type_label, type_label)*len(type_label)
      n_samples += len(type_label)

  return acc_fresh_rotten / n_samples, acc_type_label / n_samples, epoch_fresh_loss / n_samples, epoch_type_loss / n_samples

"""#Dataset and Dataloader"""

class FruitVegDataset(Dataset):
    # base_path = /content/drive/Shareddrives/testDataset/dataset
    # subpaths = [Apple, Banana, etc.]
    def __init__(self, base_path, subpaths):
        self.dataset = None
        self.type_label = []

        for idx, subpath in enumerate(subpaths):
            item_dir = os.path.join(base_path, subpath)
            item_dataset = ImageFolder(item_dir, transform=transforms.Compose([transforms.Resize([128,128]),transforms.ToTensor()]))

            # add all 3-category types to the master dataset
            if self.dataset == None:
                self.dataset = item_dataset
            else:
                self.dataset = ConcatDataset([self.dataset, item_dataset])

            # All of these have the same label
            self.type_label += [idx] * len(item_dataset)

        assert(len(self.type_label) == len(self.dataset))

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        assert(idx < self.__len__())
        return *self.dataset[idx], self.type_label[idx]

def load_data_custom(fruitList, test=False):
  data_dir = "/content/drive/Shareddrives/RotNotCrew/testDataset"
  items = fruitList
  dataset = FruitVegDataset(data_dir, items)

  if test:
    return dataset

  # define the split in percentage
  train_split, val_split, test_split = 70, 15, 15

  # get split numbers
  total_num = len(dataset)
  train_num = int(total_num*float(train_split/100))
  val_num = int(total_num*float(val_split/100))
  test_num = int(total_num*float(test_split/100))

  if total_num != train_num + val_num + test_num:
    train_num += total_num - (train_num + val_num + test_num)

  # split the dataset
  splits = [train_num, val_num, test_num]
  train_data, validate_data, test_data = random_split(dataset, splits, generator=torch.Generator().manual_seed(seed))

  return train_data, validate_data, test_data

"""# Train Model ResNet 50"""

def train_ResNet50(epochs,batch_size,lr, num_fruits_veg):
  """
  @params:
  `epochs`: number of complete passes through the training dataset.
  `batch_size`: number of samples processed before the model is updated
  `lr`: learning rate

  @desc:
  Given the epochs, batch_size, and learning rate, the functions uses the
  hyperparameters to train on a ResNet50 model, prints the Train Loss, Validate
  Loss, Validate Accuracy of each epoch, and the Test Accuracy (%). The model saves
  the model to the drive.

  """

  # initiate model
  model = ResNet50(3, num_fruits_veg)
  # using cross entropy loss function
  loss_fnc = nn.CrossEntropyLoss()
  # train model
  train_fresh_losses, train_type_losses, valid_accuracy_fresh_rotten, valid_accuracy_type, valid_fresh_losses, valid_type_losses = train_network(model, train_data, validate_data,
                                           loss_fnc, num_epochs=epochs, batch_size = batch_size,
                                           lr = lr)

  #test model
  test_accuracy_fresh_rotten, test_accuracy_type, test_fresh_losses, test_type_losses = test_network(model, test_data, loss_fnc)

  # print results
  print("Test Accuracy Fresh Rotten: {:.2f}%".format(test_accuracy_fresh_rotten * 100))
  print("Test Accuracy Type: {:.2f}%".format(test_accuracy_type * 100))

  losses = [train_fresh_losses, train_type_losses, test_fresh_losses, test_type_losses, valid_fresh_losses, valid_type_losses]
  accuracies = [valid_accuracy_fresh_rotten, valid_accuracy_type, test_accuracy_fresh_rotten, test_accuracy_type]
  return losses, accuracies

"""# Train Model ResNet18"""

def train_ResNet18(epochs,batch_size,lr, num_fruits_veg):
  """
  @params:
  `epochs`: number of complete passes through the training dataset.
  `batch_size`: number of samples processed before the model is updated
  `lr`: learning rate

  @desc:
  Given the epochs, batch_size, and learning rate, the functions uses the
  hyperparameters to train on a ResNet18 model, prints the Train Loss, Validate
  Loss, Validate Accuracy of each epoch, and the Test Accuracy (%). The model saves
  the model to the drive.

  """

  # initiate model
  model = ResNet18(3, num_fruits_veg)
    # using cross entropy loss function
  loss_fnc = nn.CrossEntropyLoss()
  # train model
  train_fresh_losses, train_type_losses, valid_accuracy_fresh_rotten, valid_accuracy_type, valid_fresh_losses, valid_type_losses = train_network(model, train_data, validate_data,
                                           loss_fnc, num_epochs=epochs, batch_size = batch_size,
                                           lr = lr)

  #test model
  test_accuracy_fresh_rotten, test_accuracy_type, test_fresh_losses, test_type_losses = test_network(model, test_data, loss_fnc)

  # print results
  print("Test Accuracy Freshness: {:.2f}%".format(test_accuracy_fresh_rotten * 100))
  print("Test Accuracy Type: {:.2f}%".format(test_accuracy_type * 100))

  losses = [train_fresh_losses, train_type_losses, test_fresh_losses, test_type_losses, valid_fresh_losses, valid_type_losses]
  accuracies = [valid_accuracy_fresh_rotten, valid_accuracy_type, test_accuracy_fresh_rotten, test_accuracy_type]
  return losses, accuracies

# For saving output
output_path = "/content/drive/Shareddrives/RotNotCrew/code/output/"
csv_file_path = output_path + "test_values.csv"
csv_file = open(csv_file_path, 'a', newline='')
writer = csv.writer(csv_file)

"""**Plot Graph**"""

def running_average(data, window):
  average_data = []
  for ind in range(len(data) - window + 1):
    average_data.append(np.mean(data[ind:ind+window]))
  for ind in range(window+1):
    average_data.insert(0, np.nan)

  return average_data

def plot_graph(losses, accuracies, info):
  # assuming only one plot is created
  new_path = output_path + "{0}_{1}_{2}_{3}".format(info[0],info[1],info[2],info[3])

  # losses = [train_fresh_losses, train_type_losses, test_fresh_losses, test_type_losses, valid_freshness_losses, valid_type_losses]
  # accuracies = [train_accuracy_fresh_rotten, train_accuracy_type, test_accuracy_fresh_rotten, test_accuracy_type]

  # plots for freshness
  train_losses = running_average(losses[0], 10)
  test_losses = losses[2].to('cpu')
  valid_losses = running_average(losses[4], 10)
  train_accuracy = running_average(accuracies[0], 10)
  test_accuracy = accuracies[2]
  plt.figure(figsize=(10, 8))
  plt.subplot(1,2,1)
  plt.plot(train_losses, label='train', marker='o', alpha=0.7)
  plt.plot(test_losses, label='test', marker='o', alpha=0.7)
  plt.plot(valid_losses, label='validate', marker='o', alpha=0.7)
  plt.grid(True)
  plt.legend()
  plt.title('Freshness Loss')
  plt.subplot(1,2,2)
  plt.plot(train_accuracy, label='validate', marker='o', alpha=0.7)
  plt.plot(test_accuracy, label='test', marker='o', alpha=0.7)
  plt.grid(True)
  plt.legend()
  plt.title('Freshness Accuracy')
  plt.savefig(new_path+"_fresh.png")

  # plots for type
  train_losses = running_average(losses[1], 10)
  test_losses = losses[3].to('cpu')
  valid_losses = running_average(losses[5], 10)
  train_accuracy = running_average(accuracies[1], 10)
  test_accuracy = accuracies[3]
  plt.figure(figsize=(10, 8))
  plt.subplot(1,2,1)
  plt.plot(train_losses, label='train', marker='o', alpha=0.7)
  plt.plot(test_losses, label='test', marker='o', alpha=0.7)
  plt.plot(valid_losses, label='validate', marker='o', alpha=0.7)
  plt.grid(True)
  plt.legend()
  plt.title('Type Loss')
  plt.subplot(1,2,2)
  plt.plot(train_accuracy, label='validate', marker='o', alpha=0.7)
  plt.plot(test_accuracy, label='test', marker='o', alpha=0.7)
  plt.grid(True)
  plt.legend()
  plt.title('Type Accuracy')
  plt.savefig(new_path+"_type.png")

"""**Set Global Parameters**"""

# global variables
seed = 5
device = "cuda" if torch.cuda.is_available() else "cpu"

"""## Train Test Tomato

Hyperparameters:\
```epochs = 100 \
batch_size = 128 \
lr=3e-4```
"""

# # set model name
# model_name = "Tomato/ResNet50_test"
# types_fruits_veg = ["Tomato"]
# num_fruits_veg = len(types_fruits_veg)
# train_data, validate_data, test_data = load_data_custom(types_fruits_veg)
# losses, accuracies = train_ResNet50(100,128,1e-4, num_fruits_veg)
# plot_graph(losses, accuracies,info = ["Tomato-ResNet50",100,128,1e-4])

"""## Train All 4

Hyperparameters:\
```epochs = 100 \
batch_size = 128 \
lr=3e-4```
"""

# set model name
model_name = "All_Four/ResNet50"
types_fruits_veg = ["Mango", "Strawberry", "Orange", "Banana","Carrot"]
num_fruits_veg = len(types_fruits_veg)
train_data, validate_data, test_data = load_data_custom(types_fruits_veg)
losses, accuracies = train_ResNet50(100,128,1e-4, num_fruits_veg)
plot_graph(losses, accuracies, info = ["All_Four-ResNet50",100,128,1e-4])

"""# Manual Evaluation"""

def test_batch(data_loader):
  run = 0
  for X, y, z in data_loader:

    # torch.no_grad() tells PyTorch to not calculate gradients to save space
    with torch.no_grad():
      X = X.to('cpu')
      # y = y.to('cpu')
      X = X.reshape(1, 3, 128, 128)
      pred_fresh, pred_type = model(X) # revised
      dist = torch.softmax(pred_fresh, -1)
      pred_label = torch.argmax(pred_fresh, -1)
      pred_type = torch.argmax(pred_type, -1)


      fig, ax = plt.subplots(figsize=(2,2))
      ax.set_xticks([]); ax.set_yticks([])

      # get text for true lable
      if int(y) == 0:
        true_label = "Fresh"
      elif int(y) == 1:
        true_label = "In-Between"
      else:
        true_label = "Rotten"

      # get text for true lable
      if int(pred_type) == 0:
        type_label = "Mango"
      elif int(pred_type) == 1:
        type_label = "Strawberry"
      elif int(pred_type) == 2:
        type_label = "Orange"
      else:
        type_label = "Banana"

      # get text for predicted label
      if int(pred_label) == 0:
        pred_label = "Fresh"
      elif int(pred_label) == 1:
        pred_label = "Rotten"
      else:
        pred_label = "In-between"

      fresh = round(float(dist[0][0] / (dist[0][0] + dist[0][1] + dist[0][2]) * 100))
      in_between = round(float(dist[0][1] / (dist[0][0] + dist[0][1]+ dist[0][2]) * 100))
      rotten =  round(float(dist[0][2] / (dist[0][0] + dist[0][1]+ dist[0][2]) * 100))

      text = fig.text(1, 1, f"True Freshness: {true_label} - Predicted Freshness: {pred_label}\nPredicted Distribution for freshness: {fresh}% fresh, {rotten}% in-between, {in_between}% rotten\nPredicted type: {type_label}",
                      horizontalalignment='center', wrap=False )
      ax.imshow(make_grid(X, nrow=1).permute(1,2,0))
    run += 1
    if run == 20:
      break

# global variables
seed = 0
device = "cuda" if torch.cuda.is_available() else "cpu"

model = ResNet50(3, 5)
model.load_state_dict(torch.load("/content/drive/Shareddrives/RotNotCrew/models/All_Four/ResNet50-70.pth", map_location=torch.device('cpu')))
model.eval()

# global variables
seed = 0
device = "cuda" if torch.cuda.is_available() else "cpu"

data_dir = "/content/drive/Shareddrives/RotNotCrew/testDataset"
types_fruits_veg = ["Test Banana"]
test_loader = load_data_custom(types_fruits_veg, test=True)
test_batch(test_loader)

csv_file.close()